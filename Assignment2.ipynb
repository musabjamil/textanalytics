{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYxKmm0RdxfB",
        "outputId": "3e3b7e0c-8f96-42b4-fc54-a25dc4a1faad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.17)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.142)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets langchain faiss-gpu pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "file_path = 'student-handbook2023-24.pdf'\n",
        "with pdfplumber.open(file_path) as pdf:\n",
        "    full_text = ''.join([page.extract_text() for page in pdf.pages])"
      ],
      "metadata": {
        "id": "-gtXUuSVsnUz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "file_size = os.path.getsize(file_path)\n",
        "file_size_mb = file_size / (1024 * 1024)\n",
        "\n",
        "print(file_size_mb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAje_UEt7NgW",
        "outputId": "02f342d4-172f-46e6-cca2-129a33da9456"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1110496520996094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(full_text)\n",
        "\n",
        "# Create sentence chunks of ~500 characters\n",
        "chunk_size = 500\n",
        "text_chunks = []\n",
        "current_chunk = \"\"\n",
        "\n",
        "for sent in doc.sents:\n",
        "    if len(current_chunk) + len(sent.text) <= chunk_size:\n",
        "        current_chunk += \" \" + sent.text\n",
        "    else:\n",
        "        text_chunks.append(current_chunk.strip())\n",
        "        current_chunk = sent.text\n",
        "\n",
        "# Add the last chunk if any text remains\n",
        "if current_chunk:\n",
        "    text_chunks.append(current_chunk.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6vObSpItQdO",
        "outputId": "e60faab3-ca21-4cc6-e926-ace1dacccff9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with pdfplumber.open(file_path) as pdf:\n",
        "    raw_text = ''\n",
        "    for page in pdf.pages:\n",
        "        raw_text += page.extract_text()\n",
        "\n",
        "chunk_size = 500\n",
        "text_chunks = [raw_text[i:i + chunk_size] for i in range(0, len(raw_text), chunk_size)]\n",
        "num_documents = len(text_chunks)\n",
        "print(f\"Number of Documents (Chunks): {num_documents}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd40ateY7awg",
        "outputId": "5543cc88-841c-4c46-e398-9d615c4b660c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Documents (Chunks): 210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mini LM (Microsoft)"
      ],
      "metadata": {
        "id": "Cv77hCwQ1cOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(text_chunks)"
      ],
      "metadata": {
        "id": "byLICFdwvE6g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Convert embeddings to a NumPy array\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "# Create a FAISS index\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)"
      ],
      "metadata": {
        "id": "GsCiaHVsvUI5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What are the requirements for maintaining academic integrity?\",\n",
        "    \"What is the attendance policy for IBA students?\",\n",
        "    \"What are the prerequisites for student applying for the Data Science program?\",\n",
        "    \"How are student assessments and grading conducted?\",\n",
        "    \"What are the consequences for violating student policies?\"\n",
        "]"
      ],
      "metadata": {
        "id": "1iTj0FXC1RKe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "# Start the overall time calculation\n",
        "start_time = time.time()\n",
        "\n",
        "# Load the QA model\n",
        "qa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2')\n",
        "\n",
        "# Assuming 'questions', 'index', and 'text_chunks' are already defined\n",
        "\n",
        "# Initialize a list to store the answers and track the total time\n",
        "answers = []\n",
        "\n",
        "# Process each question\n",
        "for question in questions:\n",
        "    # Record the start time for this particular question\n",
        "    question_start_time = time.time()\n",
        "\n",
        "    # Search the FAISS index for relevant chunks\n",
        "    query_embedding = model.encode([question])  # Encode the question\n",
        "    _, relevant_indices = index.search(query_embedding, k=3)  # Retrieve top 3 relevant chunks\n",
        "    relevant_texts = [text_chunks[i] for i in relevant_indices[0]]  # Get the corresponding text chunks\n",
        "\n",
        "    # Combine the retrieved text for the QA model\n",
        "    context = \" \".join(relevant_texts)\n",
        "\n",
        "    # Get the answer from the QA model\n",
        "    answer = qa_pipeline({'question': question, 'context': context})\n",
        "\n",
        "    # Store the answer\n",
        "    answers.append(answer['answer'])\n",
        "\n",
        "    # Record the end time for this particular question\n",
        "    question_end_time = time.time()\n",
        "\n",
        "    # Calculate the time taken for this question\n",
        "    question_time = question_end_time - question_start_time\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer['answer']}\")\n",
        "    print(f\"Time taken for this question: {question_time:.2f} seconds\")\n",
        "    print(\"\\n\" + \"=\" * 70 + \"\\n\")  # Separator for readability\n",
        "\n",
        "# Record the overall end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate total time taken for all questions\n",
        "total_time = end_time - start_time\n",
        "\n",
        "# Print the total time taken for answering all questions\n",
        "print(f\"Total time taken to answer all {len(questions)} questions: {total_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ0F5Eq7wCWD",
        "outputId": "0532139f-a333-4246-f6f6-672b07fe922e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the requirements for maintaining academic integrity?\n",
            "Answer: being honest and having strong moral principles\n",
            "Time taken for this question: 2.83 seconds\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Question: What is the attendance policy for IBA students?\n",
            "Answer: decently dressed and in a manner that is appropriate for any institution\n",
            "\n",
            "Time taken for this question: 0.88 seconds\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Question: What are the prerequisites for student applying for the Data Science program?\n",
            "Answer: 3\n",
            "Time taken for this question: 0.82 seconds\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Question: How are student assessments and grading conducted?\n",
            "Answer: o ing of their exam material\n",
            "Time taken for this question: 0.77 seconds\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Question: What are the consequences for violating student policies?\n",
            "Answer: Students are financially responsible for damages\n",
            "Time taken for this question: 0.68 seconds\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Total time taken to answer all 5 questions: 6.68 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paraphrase Mini LM (Microsoft)"
      ],
      "metadata": {
        "id": "eIog7vZs1hI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "embeddings = np.array(model.encode(text_chunks))\n",
        "\n",
        "# Create a FAISS index\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)"
      ],
      "metadata": {
        "id": "x5mmOp3DxJv6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "# Start the overall time calculation\n",
        "start_time = time.time()\n",
        "\n",
        "# Load the QA model\n",
        "qa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "# Assuming 'questions', 'index', and 'text_chunks' are already defined\n",
        "\n",
        "# Initialize a list to store the answers and track the total time\n",
        "answers = []\n",
        "\n",
        "# Process each question\n",
        "for question in questions:\n",
        "    # Record the start time for this particular question\n",
        "    question_start_time = time.time()\n",
        "\n",
        "    # Encode the question\n",
        "    query_embedding = model.encode([question])  # Encode the question\n",
        "\n",
        "    # Search the FAISS index for relevant chunks\n",
        "    _, relevant_indices = index.search(query_embedding, k=3)  # Retrieve top 3 relevant text chunks\n",
        "\n",
        "    # Extract relevant texts based on the search results\n",
        "    relevant_texts = [text_chunks[i] for i in relevant_indices[0]]  # Retrieve corresponding chunks\n",
        "\n",
        "    # Combine the retrieved texts into a single context\n",
        "    context = \" \".join(relevant_texts)\n",
        "\n",
        "    # Get the answer from the QA model\n",
        "    answer = qa_pipeline({'question': question, 'context': context})\n",
        "\n",
        "    # Store the answer\n",
        "    answers.append(answer['answer'])\n",
        "\n",
        "    # Record the end time for this particular question\n",
        "    question_end_time = time.time()\n",
        "\n",
        "    # Calculate the time taken for this question\n",
        "    question_time = question_end_time - question_start_time\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer['answer']}\")\n",
        "    print(f\"Time taken for this question: {question_time:.2f} seconds\")\n",
        "    print(\"\\n\" + \"=\" * 70 + \"\\n\")  # Separator for readability\n",
        "\n",
        "# Record the overall end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate total time taken for all questions\n",
        "total_time = end_time - start_time\n",
        "\n",
        "# Print the total time taken for answering all questions\n",
        "print(f\"Total time taken to answer all {len(questions)} questions: {total_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS-ZqpIIxQfZ",
        "outputId": "44ae3c4a-46bf-4ced-cfb7-d70df79e9757"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the requirements for maintaining academic integrity?\n",
            "Answer: Students should be honest about their identity\n",
            "Time taken for this question: 8.42 seconds\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Question: What is the attendance policy for IBA students?\n",
            "Answer: Students are expected to:\n",
            "l Present themselves as mature\n",
            "Time taken for this question: 3.35 seconds\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Question: What are the prerequisites for student applying for the Data Science program?\n",
            "Answer: high-performance computing to commutative algebra\n",
            "Time taken for this question: 2.28 seconds\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Question: How are student assessments and grading conducted?\n",
            "Answer: fill the faculty and course evaluation\n",
            "questionnaires\n",
            "Time taken for this question: 2.20 seconds\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Question: What are the consequences for violating student policies?\n",
            "Answer: disciplinary action against the IBA\n",
            "Time taken for this question: 2.50 seconds\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Total time taken to answer all 5 questions: 19.70 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('t5-small')\n",
        "embeddings = np.array(model.encode(text_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acFs8IuT2LA7",
        "outputId": "20ce2180-a85c-4a99-8815-ecb3e1403a5d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name t5-small. Creating a new one with mean pooling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import T5Tokenizer\n",
        "import time\n",
        "\n",
        "# Load the T5-small model for question answering\n",
        "qa_pipeline = pipeline('text2text-generation', model='t5-small')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "# Track the start time of the whole process\n",
        "start_time = time.time()\n",
        "\n",
        "# Maximum token length (you can adjust this based on your model's limit)\n",
        "max_input_length = 512  # You can adjust this based on your model's max token limit\n",
        "\n",
        "# Process each question\n",
        "for question in questions:\n",
        "    # Start timing for each question\n",
        "    question_start_time = time.time()\n",
        "\n",
        "    # Combine the question and context (use FAISS to retrieve relevant chunks if needed)\n",
        "    context = \" \".join(text_chunks)  # or use the top relevant chunks from FAISS search\n",
        "\n",
        "    # Encode the input to count tokens and truncate if necessary\n",
        "    input_text = f\"question: {question} context: {context}\"\n",
        "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # If the number of tokens exceeds the max length, truncate\n",
        "    if len(input_tokens[0]) > max_input_length:\n",
        "        input_tokens = input_tokens[:, :max_input_length]\n",
        "\n",
        "    # Decode back to text\n",
        "    truncated_input_text = tokenizer.decode(input_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    # Get the answer using T5\n",
        "    answer = qa_pipeline(truncated_input_text)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer[0]['generated_text']}\")\n",
        "    print(\"\\n\" + \"=\" * 70 + \"\\n\")  # Separator for readability\n",
        "\n",
        "    # Track time taken for the current question\n",
        "    question_end_time = time.time()\n",
        "    question_time = question_end_time - question_start_time\n",
        "    print(f\"Time taken for this question: {question_time:.2f} seconds\")\n",
        "\n",
        "# Track total time taken for all questions\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print(f\"Total time taken for processing all questions: {total_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqIbpvtS2NrT",
        "outputId": "acc0a5c0-63a9-4fdf-e581-6d8db603370a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (24294 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the requirements for maintaining academic integrity?\n",
            "Answer: a not to be regarded as an roadmap to assist you in irrevocable contract\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Time taken for this question: 1.09 seconds\n",
            "Question: What is the attendance policy for IBA students?\n",
            "Answer: 05 About IBA privileges\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Time taken for this question: 1.00 seconds\n",
            "Question: What are the prerequisites for student applying for the Data Science program?\n",
            "Answer: 05 About IBA privileges\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Time taken for this question: 1.03 seconds\n",
            "Question: How are student assessments and grading conducted?\n",
            "Answer: it is your academic grading policies, 07 School of Economics and Social Sciences responsibility to ensure\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Time taken for this question: 1.45 seconds\n",
            "Question: What are the consequences for violating student policies?\n",
            "Answer: the Institution\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Time taken for this question: 0.60 seconds\n",
            "Total time taken for processing all questions: 5.17 seconds\n"
          ]
        }
      ]
    }
  ]
}